ğŸ—‚ï¸ Case 1: Hiring Bot Bias
âœ… Whatâ€™s happening:

A company is using an AI-powered tool to screen job applications. The AI analyzes rÃ©sumÃ©s and automatically selects candidates to move forward to interviews.

ğŸš¨ Whatâ€™s problematic:

The AI system tends to reject more female applicants, especially those who have career gaps â€” often due to maternity leave or caregiving responsibilities. This indicates gender bias, likely caused by the AI being trained on historical hiring data that contains existing human bias.

This raises major concerns about fairness and discrimination, especially in high-stakes situations like employment.

ğŸ’¡ One improvement idea:

The company should audit the training data for bias and adjust the algorithm to ignore career gaps unless they are job-critical. Additionally, the hiring AI should be evaluated regularly for fairness, with transparency about how it makes decisions.

ğŸ—‚ï¸ Case 2: Overzealous School Proctoring AI
âœ… Whatâ€™s happening:

A school uses AI proctoring software during online exams. It uses a studentâ€™s webcam to track eye movement and facial behavior to flag potential cheating.

ğŸš¨ Whatâ€™s problematic:

The system often falsely flags neurodivergent students (e.g., those with ADHD or autism) as cheating, due to natural behaviors like fidgeting, avoiding eye contact, or looking away from the screen. The AI assumes all students behave the same way when concentrating â€” which is inaccurate and exclusionary.

This raises concerns about accessibility, inclusivity, and false accusations.

ğŸ’¡ One improvement idea:

The school should allow students to disclose accessibility needs and adjust how the AI interprets behavior accordingly. Even better, they could shift toward open-book or project-based assessments to reduce reliance on surveillance-heavy tools.

ğŸ¨ Bonus Blog Post: When AI Gets It Wrong: Two Cases That Need a Rethink

Title: Busted by a Bot: Two AI Fails That Need a Fix
By: A Responsible AI Detective ğŸ•µï¸â€â™‚ï¸

Letâ€™s face it â€” AI is cool. But when it comes to real-world decisions, it doesnâ€™t always play fair. I put on my detective hat and found two AI cases that could seriously mess with peopleâ€™s lives. Here's what I uncovered.

ğŸ” Case #1: The Biased Hiring Bot
This bot is meant to help companies pick job candidates faster â€” but it's secretly ghosting women with career gaps. Why? Because it learned from past hiring decisions, and history isnâ€™t exactly feminist-friendly. Spoiler: Career breaks donâ€™t mean lack of skill.

ğŸ§  Fix it: Clean the data. Train the bot to focus on skills, not time off.

ğŸ‘€ Case #2: The Overwatch Exam AI
Online proctoring software is flagging students as cheaters for looking away or fidgeting. Sounds fairâ€¦ until you realize itâ€™s targeting neurodivergent students who just think and focus differently.

ğŸ§  Fix it: Let students flag accessibility needs, or better yet, ditch the robot spy and redesign how we test.
